{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch-pretrained-BERT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "qoAtukMKD4e9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# [tokenization.py](https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/pytorch_pretrained_bert/tokenization.py)"
      ]
    },
    {
      "metadata": {
        "id": "Pk6EawsJEDFl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import"
      ]
    },
    {
      "metadata": {
        "id": "dcEm1LkhEB4u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import unicodedata\n",
        "import os\n",
        "import logging\n",
        "\n",
        "# from .file_utils import cached_path // import 오류"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eUee3BWdETX1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Configuration"
      ]
    },
    {
      "metadata": {
        "id": "TQR432lnEPQA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "PRETRAINED_VOCAB_ARCHIVE_MAP = {\n",
        "    'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\",\n",
        "    'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt\",\n",
        "    'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt\",\n",
        "    'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt\",\n",
        "    'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt\",\n",
        "    'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt\",\n",
        "    'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt\",\n",
        "}\n",
        "PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP = {\n",
        "    'bert-base-uncased': 512,\n",
        "    'bert-large-uncased': 512,\n",
        "    'bert-base-cased': 512,\n",
        "    'bert-large-cased': 512,\n",
        "    'bert-base-multilingual-uncased': 512,\n",
        "    'bert-base-multilingual-cased': 512,\n",
        "    'bert-base-chinese': 512,\n",
        "}\n",
        "VOCAB_NAME = 'vocab.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lHRgjRqDOOr3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## load_vocab 함수 정의\n"
      ]
    },
    {
      "metadata": {
        "id": "PWsHZq3tEYdS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_vocab(vocab_file):\n",
        "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
        "    vocab = collections.OrderedDict()\n",
        "    index = 0\n",
        "    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
        "        while True:\n",
        "            token = reader.readline()\n",
        "            if not token:\n",
        "                break\n",
        "            token = token.strip() ## white space 제거\n",
        "            vocab[token] = index\n",
        "            index += 1\n",
        "    return vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FubBIUv3OUq7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## whitespace_tokenize 함수 정의"
      ]
    },
    {
      "metadata": {
        "id": "vhYI56JfEmzw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def whitespace_tokenize(text):\n",
        "    \"\"\"Runs basic whitespace cleaning and splitting on a peice of text.\"\"\"\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    tokens = text.split() ## text를 문자열 단위로 나눔\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fByk3m1kOaYf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BertTokenizer class 정의\n",
        "\n",
        "[OrderedDict](https://godoftyping.wordpress.com/2015/05/13/python-%EC%88%9C%EC%84%9C%EB%A5%BC-%EA%B8%B0%EC%96%B5%ED%95%98%EB%8A%94-%EC%82%AC%EC%A0%84%ED%98%95-ordereddict/),\n",
        "[가변인자](https://mingrammer.com/understanding-the-asterisk-of-python/)"
      ]
    },
    {
      "metadata": {
        "id": "qw2tJr8wEuSW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertTokenizer(object):\n",
        "    \"\"\"Runs end-to-end tokenization: punctuation splitting + wordpiece\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_file, do_lower_case=True, max_len=None,\n",
        "                 never_split=(\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\")):\n",
        "        if not os.path.isfile(vocab_file):\n",
        "            raise ValueError(\n",
        "                \"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained \"\n",
        "                \"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(vocab_file))\n",
        "        self.vocab = load_vocab(vocab_file)\n",
        "        self.ids_to_tokens = collections.OrderedDict(\n",
        "            [(ids, tok) for tok, ids in self.vocab.items()])\n",
        "        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case,\n",
        "                                              never_split=never_split)\n",
        "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
        "        self.max_len = max_len if max_len is not None else int(1e12)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        split_tokens = []\n",
        "        for token in self.basic_tokenizer.tokenize(text):\n",
        "            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
        "                split_tokens.append(sub_token)\n",
        "        return split_tokens\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "        ## token을 vocab에 있는 id로 변환\n",
        "        \"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"\n",
        "        ids = []\n",
        "        for token in tokens:\n",
        "            ids.append(self.vocab[token])\n",
        "        if len(ids) > self.max_len:\n",
        "            raise ValueError(\n",
        "                \"Token indices sequence length is longer than the specified maximum \"\n",
        "                \" sequence length for this BERT model ({} > {}). Running this\"\n",
        "                \" sequence through BERT will result in indexing errors\".format(len(ids), self.max_len)\n",
        "            )\n",
        "        return ids\n",
        "\n",
        "    def convert_ids_to_tokens(self, ids):\n",
        "        ## vocab id를 token으로 변환\n",
        "        \"\"\"Converts a sequence of ids in wordpiece tokens using the vocab.\"\"\"\n",
        "        tokens = []\n",
        "        for i in ids:\n",
        "            tokens.append(self.ids_to_tokens[i])\n",
        "        return tokens\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name, cache_dir=None, *inputs, **kwargs):\n",
        "        ## 모델 이름을 분석하여 다운로드 받아서 BertTokenizer class를 생성한다.\n",
        "        \"\"\"\n",
        "        Instantiate a PreTrainedBertModel from a pre-trained model file.\n",
        "        Download and cache the pre-trained model file if needed.\n",
        "        \"\"\"\n",
        "        if pretrained_model_name in PRETRAINED_VOCAB_ARCHIVE_MAP:\n",
        "            vocab_file = PRETRAINED_VOCAB_ARCHIVE_MAP[pretrained_model_name]\n",
        "        else:\n",
        "            vocab_file = pretrained_model_name\n",
        "        if os.path.isdir(vocab_file):\n",
        "            vocab_file = os.path.join(vocab_file, VOCAB_NAME)\n",
        "        # redirect to the cache, if necessary\n",
        "        try:\n",
        "            resolved_vocab_file = cached_path(vocab_file, cache_dir=cache_dir)\n",
        "        except FileNotFoundError:\n",
        "            logger.error(\n",
        "                \"Model name '{}' was not found in model name list ({}). \"\n",
        "                \"We assumed '{}' was a path or url but couldn't find any file \"\n",
        "                \"associated to this path or url.\".format(\n",
        "                    pretrained_model_name,\n",
        "                    ', '.join(PRETRAINED_VOCAB_ARCHIVE_MAP.keys()),\n",
        "                    vocab_file))\n",
        "            return None\n",
        "        if resolved_vocab_file == vocab_file:\n",
        "            logger.info(\"loading vocabulary file {}\".format(vocab_file))\n",
        "        else:\n",
        "            logger.info(\"loading vocabulary file {} from cache at {}\".format(\n",
        "                vocab_file, resolved_vocab_file))\n",
        "        if pretrained_model_name in PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP:\n",
        "            # if we're using a pretrained model, ensure the tokenizer wont index sequences longer\n",
        "            # than the number of positional embeddings\n",
        "            max_len = PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP[pretrained_model_name]\n",
        "            kwargs['max_len'] = min(kwargs.get('max_len', int(1e12)), max_len)\n",
        "        # Instantiate tokenizer.\n",
        "        tokenizer = cls(resolved_vocab_file, *inputs, **kwargs)\n",
        "        return tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e9pWxxT7baqd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BasicTokenizer class 정의\n",
        "\n",
        "[ord](https://docs.python.org/ko/3/library/functions.html#ord),\n",
        "[unicodedata.normalize](https://docs.python.org/2/library/unicodedata.html#unicodedata.normalize),\n",
        "[Mn list](https://www.fileformat.info/info/unicode/category/Mn/list.htm)\n"
      ]
    },
    {
      "metadata": {
        "id": "m0DEWrPbEwT0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BasicTokenizer(object):\n",
        "    \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 do_lower_case=True,\n",
        "                 never_split=(\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\")):\n",
        "        \"\"\"Constructs a BasicTokenizer.\n",
        "        Args:\n",
        "          do_lower_case: Whether to lower case the input.\n",
        "        \"\"\"\n",
        "        self.do_lower_case = do_lower_case\n",
        "        self.never_split = never_split\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenizes a piece of text.\"\"\"\n",
        "        ## 제어문자 제거 및 whitespace 처리\n",
        "        text = self._clean_text(text)\n",
        "        # This was added on November 1st, 2018 for the multilingual and Chinese\n",
        "        # models. This is also applied to the English models now, but it doesn't\n",
        "        # matter since the English models were not trained on any Chinese data\n",
        "        # and generally don't have any Chinese data in them (there are Chinese\n",
        "        # characters in the vocabulary because Wikipedia does have some Chinese\n",
        "        # words in the English Wikipedia.).\n",
        "        text = self._tokenize_chinese_chars(text)\n",
        "        orig_tokens = whitespace_tokenize(text)\n",
        "        split_tokens = []\n",
        "        for token in orig_tokens:\n",
        "            if self.do_lower_case and token not in self.never_split:\n",
        "                token = token.lower()\n",
        "                token = self._run_strip_accents(token)\n",
        "            split_tokens.extend(self._run_split_on_punc(token))\n",
        "\n",
        "        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
        "        return output_tokens\n",
        "\n",
        "    def _run_strip_accents(self, text):\n",
        "        \"\"\"Strips accents from a piece of text.\"\"\"\n",
        "        text = unicodedata.normalize(\"NFD\", text)\n",
        "        output = []\n",
        "        for char in text:\n",
        "            cat = unicodedata.category(char)\n",
        "            if cat == \"Mn\": ## [Mn]\tMark, Nonspacing\n",
        "                continue\n",
        "            output.append(char)\n",
        "        return \"\".join(output)\n",
        "\n",
        "    def _run_split_on_punc(self, text):\n",
        "        \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
        "        if text in self.never_split:\n",
        "            return [text]\n",
        "        chars = list(text)\n",
        "        i = 0\n",
        "        start_new_word = True\n",
        "        output = []\n",
        "        while i < len(chars):\n",
        "            char = chars[i]\n",
        "            if _is_punctuation(char):\n",
        "                output.append([char])\n",
        "                start_new_word = True\n",
        "            else:\n",
        "                if start_new_word:\n",
        "                    output.append([])\n",
        "                start_new_word = False\n",
        "                output[-1].append(char)\n",
        "            i += 1\n",
        "\n",
        "        return [\"\".join(x) for x in output]\n",
        "\n",
        "    def _tokenize_chinese_chars(self, text):\n",
        "        \"\"\"Adds whitespace around any CJK character.\"\"\"\n",
        "        output = []\n",
        "        for char in text:\n",
        "            cp = ord(char)\n",
        "            if self._is_chinese_char(cp):\n",
        "                ## 중국어 문자는 분리 함\n",
        "                output.append(\" \")\n",
        "                output.append(char)\n",
        "                output.append(\" \")\n",
        "            else:\n",
        "                output.append(char)\n",
        "        return \"\".join(output)\n",
        "\n",
        "    def _is_chinese_char(self, cp):\n",
        "        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n",
        "        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n",
        "        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n",
        "        #\n",
        "        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n",
        "        # despite its name. The modern Korean Hangul alphabet is a different block,\n",
        "        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n",
        "        # space-separated words, so they are not treated specially and handled\n",
        "        # like the all of the other languages.\n",
        "        ## 한국어는 지원이 안됨\n",
        "        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n",
        "                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n",
        "                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n",
        "                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n",
        "                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n",
        "                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n",
        "                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n",
        "                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _clean_text(self, text):\n",
        "        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
        "        output = []\n",
        "        for char in text:\n",
        "            cp = ord(char)\n",
        "            if cp == 0 or cp == 0xfffd or _is_control(char):\n",
        "                ## 코드값 0, 0xfffd, 제어문자는 제거 함\n",
        "                continue\n",
        "            if _is_whitespace(char):\n",
        "                ## whitespace는 모두 space로 변경 함\n",
        "                output.append(\" \")\n",
        "            else:\n",
        "                output.append(char)\n",
        "        return \"\".join(output)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gxIEvgVciYn6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## WordpieceTokenizer class 정의\n",
        "\n",
        "### input = \"unaffable\" --> output = [\"un\", \"##aff\", \"##able\"]\n",
        "\n",
        "vocabulary에 없는 단어를 위 형식처럼 있는 단위로 쪼갬"
      ]
    },
    {
      "metadata": {
        "id": "NroXvceaE6p7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class WordpieceTokenizer(object):\n",
        "    \"\"\"Runs WordPiece tokenization.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=100):\n",
        "        self.vocab = vocab\n",
        "        self.unk_token = unk_token\n",
        "        self.max_input_chars_per_word = max_input_chars_per_word\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenizes a piece of text into its word pieces.\n",
        "        This uses a greedy longest-match-first algorithm to perform tokenization\n",
        "        using the given vocabulary.\n",
        "        For example:\n",
        "          input = \"unaffable\"\n",
        "          output = [\"un\", \"##aff\", \"##able\"]\n",
        "        Args:\n",
        "          text: A single token or whitespace separated tokens. This should have\n",
        "            already been passed through `BasicTokenizer`.\n",
        "        Returns:\n",
        "          A list of wordpiece tokens.\n",
        "        \"\"\"\n",
        "\n",
        "        output_tokens = []\n",
        "        for token in whitespace_tokenize(text):\n",
        "            chars = list(token)\n",
        "            if len(chars) > self.max_input_chars_per_word:\n",
        "                output_tokens.append(self.unk_token)\n",
        "                continue\n",
        "\n",
        "            is_bad = False\n",
        "            start = 0\n",
        "            sub_tokens = []\n",
        "            while start < len(chars):\n",
        "                end = len(chars)\n",
        "                cur_substr = None\n",
        "                while start < end:\n",
        "                    substr = \"\".join(chars[start:end])\n",
        "                    if start > 0:\n",
        "                        substr = \"##\" + substr\n",
        "                    if substr in self.vocab:\n",
        "                        cur_substr = substr\n",
        "                        break\n",
        "                    end -= 1\n",
        "                if cur_substr is None:\n",
        "                    is_bad = True\n",
        "                    break\n",
        "                sub_tokens.append(cur_substr)\n",
        "                start = end\n",
        "\n",
        "            if is_bad:\n",
        "                output_tokens.append(self.unk_token)\n",
        "            else:\n",
        "                output_tokens.extend(sub_tokens)\n",
        "        return output_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DqkA9dmLiJaI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## _is_whitespace 함수 정의\n",
        "\n",
        "char가 whitespace 문자인지 여부 확인 함"
      ]
    },
    {
      "metadata": {
        "id": "uHr9z2pDE-e4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _is_whitespace(char):\n",
        "    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
        "    # \\t, \\n, and \\r are technically contorl characters but we treat them\n",
        "    # as whitespace since they are generally considered as such.\n",
        "    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "        return True\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat == \"Zs\":\n",
        "        return True\n",
        "    return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "65XQvFTlb8Dd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## _is_control 함수 정의\n",
        "char가 제어문자인지 여부 확인 함\n",
        "\n",
        "[unicodedata.category](https://docs.python.org/3.7/library/unicodedata.html#unicodedata.category),\n",
        "[카테고리 종류](https://www.fileformat.info/info/unicode/category/index.htm)"
      ]
    },
    {
      "metadata": {
        "id": "06PeGwWUFA3E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _is_control(char):\n",
        "    \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
        "    # These are technically control characters but we count them as whitespace\n",
        "    # characters.\n",
        "    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "        return False\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat.startswith(\"C\"):\n",
        "        return True\n",
        "    return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u5_7ECotgyN3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## _is_punctuation 함수 정의\n",
        "\n",
        "char가 punctuation(특수문자) 인지 학인함\n",
        "\n",
        "[ASCII 코드표](https://ko.wikipedia.org/wiki/ASCII)"
      ]
    },
    {
      "metadata": {
        "id": "WaGAKhY1FB9p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _is_punctuation(char):\n",
        "    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
        "    cp = ord(char)\n",
        "    # We treat all non-letter/number ASCII as punctuation.\n",
        "    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
        "    # Punctuation class but we treat them as punctuation anyways, for\n",
        "    # consistency.\n",
        "    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
        "            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)): ## non-letter/number ASCII\n",
        "        return True\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat.startswith(\"P\"): ## punctuation\n",
        "        return True\n",
        "    return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s0toHYNeYKla",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# [optimization.py](https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/pytorch_pretrained_bert/optimization.py)\n"
      ]
    },
    {
      "metadata": {
        "id": "mZQvXg5ba5gn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.optimizer import required\n",
        "from torch.nn.utils import clip_grad_norm_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tJ4TfDCja9KK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def warmup_cosine(x, warmup=0.002):\n",
        "    if x < warmup:\n",
        "        return x/warmup\n",
        "    return 0.5 * (1.0 + torch.cos(math.pi * x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NjjVmY1rbBC1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def warmup_constant(x, warmup=0.002):\n",
        "    if x < warmup:\n",
        "        return x/warmup\n",
        "    return 1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cu9sAcnYbDSj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def warmup_linear(x, warmup=0.002):\n",
        "    if x < warmup:\n",
        "        return x/warmup\n",
        "    return 1.0 - x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VVGtcrH5bF9p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SCHEDULES = {\n",
        "    'warmup_cosine':warmup_cosine,\n",
        "    'warmup_constant':warmup_constant,\n",
        "    'warmup_linear':warmup_linear,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bBUAJ4A8bdFy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BertAdam class 정의\n",
        "\n",
        "[torch.optim.Optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer),\n",
        "[torch.nn.utils.clip_grad_norm_](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.clip_grad_norm_),\n",
        "[torch.mul](https://pytorch.org/docs/stable/torch.html#torch.mul),\n",
        "[torch.add](https://pytorch.org/docs/stable/torch.html#torch.add),\n",
        "[torch.addcmul](https://pytorch.org/docs/stable/torch.html#torch.addcmul)\n",
        "\n",
        "아래내용은 현재로써는 파악이 잘 안됨 (추후 확인 필요)"
      ]
    },
    {
      "metadata": {
        "id": "kxHrDZQ7a5Eg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertAdam(Optimizer):\n",
        "    \"\"\"Implements BERT version of Adam algorithm with weight decay fix.\n",
        "    Params:\n",
        "        lr: learning rate\n",
        "        warmup: portion of t_total for the warmup, -1  means no warmup. Default: -1\n",
        "        t_total: total number of training steps for the learning\n",
        "            rate schedule, -1  means constant learning rate. Default: -1\n",
        "        schedule: schedule to use for the warmup (see above). Default: 'warmup_linear'\n",
        "        b1: Adams b1. Default: 0.9\n",
        "        b2: Adams b2. Default: 0.999\n",
        "        e: Adams epsilon. Default: 1e-6\n",
        "        weight_decay: Weight decay. Default: 0.01\n",
        "        max_grad_norm: Maximum norm for the gradients (-1 means no clipping). Default: 1.0\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr=required, warmup=-1, t_total=-1, schedule='warmup_linear',\n",
        "                 b1=0.9, b2=0.999, e=1e-6, weight_decay=0.01,\n",
        "                 max_grad_norm=1.0):\n",
        "        if lr is not required and lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n",
        "        if schedule not in SCHEDULES:\n",
        "            raise ValueError(\"Invalid schedule parameter: {}\".format(schedule))\n",
        "        if not 0.0 <= warmup < 1.0 and not warmup == -1:\n",
        "            raise ValueError(\"Invalid warmup: {} - should be in [0.0, 1.0[ or -1\".format(warmup))\n",
        "        if not 0.0 <= b1 < 1.0:\n",
        "            raise ValueError(\"Invalid b1 parameter: {} - should be in [0.0, 1.0[\".format(b1))\n",
        "        if not 0.0 <= b2 < 1.0:\n",
        "            raise ValueError(\"Invalid b2 parameter: {} - should be in [0.0, 1.0[\".format(b2))\n",
        "        if not e >= 0.0:\n",
        "            raise ValueError(\"Invalid epsilon value: {} - should be >= 0.0\".format(e))\n",
        "        defaults = dict(lr=lr, schedule=schedule, warmup=warmup, t_total=t_total,\n",
        "                        b1=b1, b2=b2, e=e, weight_decay=weight_decay,\n",
        "                        max_grad_norm=max_grad_norm)\n",
        "        super(BertAdam, self).__init__(params, defaults)\n",
        "\n",
        "    def get_lr(self):\n",
        "        lr = []\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:\n",
        "                    return [0]\n",
        "                if group['t_total'] != -1:\n",
        "                    schedule_fct = SCHEDULES[group['schedule']]\n",
        "                    lr_scheduled = group['lr'] * schedule_fct(state['step']/group['t_total'], group['warmup'])\n",
        "                else:\n",
        "                    lr_scheduled = group['lr']\n",
        "                lr.append(lr_scheduled)\n",
        "        return lr\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                ### 초기화\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['next_m'] = torch.zeros_like(p.data)\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['next_v'] = torch.zeros_like(p.data)\n",
        "\n",
        "                next_m, next_v = state['next_m'], state['next_v']\n",
        "                beta1, beta2 = group['b1'], group['b2']\n",
        "\n",
        "                # Add grad clipping\n",
        "                if group['max_grad_norm'] > 0:\n",
        "                    clip_grad_norm_(p, group['max_grad_norm'])\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                # In-place operations to update the averages at the same time\n",
        "                next_m.mul_(beta1).add_(1 - beta1, grad)\n",
        "                next_v.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                update = next_m / (next_v.sqrt() + group['e'])\n",
        "\n",
        "                # Just adding the square of the weights to the loss function is *not*\n",
        "                # the correct way of using L2 regularization/weight decay with Adam,\n",
        "                # since that will interact with the m and v parameters in strange ways.\n",
        "                #\n",
        "                # Instead we want to decay the weights in a manner that doesn't interact\n",
        "                # with the m/v parameters. This is equivalent to adding the square\n",
        "                # of the weights to the loss with plain (non-momentum) SGD.\n",
        "                if group['weight_decay'] > 0.0:\n",
        "                    update += group['weight_decay'] * p.data\n",
        "\n",
        "                if group['t_total'] != -1:\n",
        "                    schedule_fct = SCHEDULES[group['schedule']]\n",
        "                    lr_scheduled = group['lr'] * schedule_fct(state['step']/group['t_total'], group['warmup'])\n",
        "                else:\n",
        "                    lr_scheduled = group['lr']\n",
        "\n",
        "                update_with_lr = lr_scheduled * update\n",
        "                p.data.add_(-update_with_lr)\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                # step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1\n",
        "                # No bias correction\n",
        "                # bias_correction1 = 1 - beta1 ** state['step']\n",
        "                # bias_correction2 = 1 - beta2 ** state['step']\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q4m1y-irh27F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# [modeling.py](https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/pytorch_pretrained_bert/modeling.py)"
      ]
    },
    {
      "metadata": {
        "id": "QEVRf_0xpiPP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import"
      ]
    },
    {
      "metadata": {
        "id": "DXVxtS9ultMc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import copy\n",
        "import json\n",
        "import math\n",
        "import logging\n",
        "import tarfile\n",
        "import tempfile\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "# from .file_utils import cached_path // import 오류"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_0faBhhzplOW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Configuration\n",
        "- bert_config.json, pytorch_model.bin은 파일을 다운 받아 압축파일을 풀면 확인 가능\n",
        "- bert_config.json: a configuration file for the model\n",
        "- pytorch_model.bin: a PyTorch dump of a BertForPreTraining instance"
      ]
    },
    {
      "metadata": {
        "id": "grl0YjJimURm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "PRETRAINED_MODEL_ARCHIVE_MAP = {\n",
        "    'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz\",\n",
        "    'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz\",\n",
        "    'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz\",\n",
        "    'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz\",\n",
        "    'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased.tar.gz\",\n",
        "    'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased.tar.gz\",\n",
        "    'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz\",\n",
        "}\n",
        "CONFIG_NAME = 'bert_config.json'\n",
        "WEIGHTS_NAME = 'pytorch_model.bin'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SZNHghK1m8u2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Gelu 함수 선언\n",
        "\n",
        "![대체 텍스트](https://www.groundai.com/media/arxiv_projects/137817/x1.png.750x0_q75_crop.png)\n",
        "\n",
        "[torch.erf](https://pytorch.org/docs/stable/torch.html#torch.erf)\n"
      ]
    },
    {
      "metadata": {
        "id": "YGDGkODFmcnb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gelu(x):\n",
        "    \"\"\"Implementation of the gelu activation function.\n",
        "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X-wv5Sb7o43V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Swish 함수 선언\n",
        "\n",
        "![대체 텍스트](https://lazyprogrammer.me/wp-content/uploads/2017/10/Screen-Shot-2017-10-18-at-2.39.55-PM-300x130.png)"
      ]
    },
    {
      "metadata": {
        "id": "Sn4fvSpeoS7O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yp-tXmDapONh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Activation Function 저장"
      ]
    },
    {
      "metadata": {
        "id": "Rtz4CGF6pL3A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ACT2FN = {\"gelu\": gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fj2vBOPUq4PN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BertConfig class 정의"
      ]
    },
    {
      "metadata": {
        "id": "c-UHN17Iq7i8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertConfig(object):\n",
        "    \"\"\"Configuration class to store the configuration of a `BertModel`.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 vocab_size_or_config_json_file,\n",
        "                 hidden_size=768,\n",
        "                 num_hidden_layers=12,\n",
        "                 num_attention_heads=12,\n",
        "                 intermediate_size=3072,\n",
        "                 hidden_act=\"gelu\",\n",
        "                 hidden_dropout_prob=0.1,\n",
        "                 attention_probs_dropout_prob=0.1,\n",
        "                 max_position_embeddings=512,\n",
        "                 type_vocab_size=2,\n",
        "                 initializer_range=0.02):\n",
        "        \"\"\"Constructs BertConfig.\n",
        "        Args:\n",
        "            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n",
        "            hidden_size: Size of the encoder layers and the pooler layer.\n",
        "            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n",
        "            num_attention_heads: Number of attention heads for each attention layer in\n",
        "                the Transformer encoder.\n",
        "            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n",
        "                layer in the Transformer encoder.\n",
        "            hidden_act: The non-linear activation function (function or string) in the\n",
        "                encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n",
        "            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n",
        "                layers in the embeddings, encoder, and pooler.\n",
        "            attention_probs_dropout_prob: The dropout ratio for the attention\n",
        "                probabilities.\n",
        "            max_position_embeddings: The maximum sequence length that this model might\n",
        "                ever be used with. Typically set this to something large just in case\n",
        "                (e.g., 512 or 1024 or 2048).\n",
        "            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n",
        "                `BertModel`.\n",
        "            initializer_range: The sttdev of the truncated_normal_initializer for\n",
        "                initializing all weight matrices.\n",
        "        \"\"\"\n",
        "        ###################################################################\n",
        "        # bert_config.json 파일을 읽어서 self.__dict__에 저장\n",
        "        ###################################################################\n",
        "        if isinstance(vocab_size_or_config_json_file, str):\n",
        "            with open(vocab_size_or_config_json_file, \"r\", encoding='utf-8') as reader:\n",
        "                json_config = json.loads(reader.read())\n",
        "            for key, value in json_config.items():\n",
        "                self.__dict__[key] = value\n",
        "        ###################################################################\n",
        "        # 입력값을 사용\n",
        "        ###################################################################\n",
        "        elif isinstance(vocab_size_or_config_json_file, int):\n",
        "            self.vocab_size = vocab_size_or_config_json_file\n",
        "            self.hidden_size = hidden_size\n",
        "            self.num_hidden_layers = num_hidden_layers\n",
        "            self.num_attention_heads = num_attention_heads\n",
        "            self.hidden_act = hidden_act\n",
        "            self.intermediate_size = intermediate_size\n",
        "            self.hidden_dropout_prob = hidden_dropout_prob\n",
        "            self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "            self.max_position_embeddings = max_position_embeddings\n",
        "            self.type_vocab_size = type_vocab_size\n",
        "            self.initializer_range = initializer_range\n",
        "        else:\n",
        "            raise ValueError(\"First argument must be either a vocabulary size (int)\"\n",
        "                             \"or the path to a pretrained model config file (str)\")\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, json_object):\n",
        "        \"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"\n",
        "        ###################################################################\n",
        "        # BertConfig class에 json_object 값 저장\n",
        "        ###################################################################\n",
        "        config = BertConfig(vocab_size_or_config_json_file=-1)\n",
        "        for key, value in json_object.items():\n",
        "            config.__dict__[key] = value\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_json_file(cls, json_file):\n",
        "        \"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\n",
        "        ###################################################################\n",
        "        # file로 부터 json을 만든 후 from_dict 호출\n",
        "        ###################################################################\n",
        "        with open(json_file, \"r\", encoding='utf-8') as reader:\n",
        "            text = reader.read()\n",
        "        return cls.from_dict(json.loads(text))\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EJWvqt7vPF8e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BertLayerNorm class 정의\n",
        "apex.normalization.fused_layer_norm.FusedLayerNorm 를 사용하거나 재 정의\n",
        "\n",
        "[FusedLayerNorm](https://nvidia.github.io/apex/layernorm.html?highlight=fusedlayernorm),\n",
        "[torch.ones](https://pytorch.org/docs/stable/torch.html#torch.ones),\n",
        "[torch.zeros](https://pytorch.org/docs/stable/torch.html#torch.zeros),\n",
        "[torch.mean](https://pytorch.org/docs/stable/torch.html#torch.mean),\n",
        "[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)"
      ]
    },
    {
      "metadata": {
        "id": "--I4mIw0PEsF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from apex.normalization.fused_layer_norm import FusedLayerNorm as BertLayerNorm\n",
        "except ImportError:\n",
        "    print(\"Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\")\n",
        "    class BertLayerNorm(nn.Module):\n",
        "        def __init__(self, hidden_size, eps=1e-12):\n",
        "            \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
        "            \"\"\"\n",
        "            super(BertLayerNorm, self).__init__()\n",
        "            self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "            self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
        "            self.variance_epsilon = eps\n",
        "\n",
        "        def forward(self, x):\n",
        "            u = x.mean(-1, keepdim=True) ### 평균\n",
        "            s = (x - u).pow(2).mean(-1, keepdim=True) ### 분산? 평균과 차이의 제곱의 평균\n",
        "            x = (x - u) / torch.sqrt(s + self.variance_epsilon) ### 평균과 차이 / 제곱근 (분산? + epsilion)\n",
        "            return self.weight * x + self.bias ### W * X + b값"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7PGSJFbkRpno",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BertEmbeddings class 정의\n",
        "\n",
        "[nn.Embedding](https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding),\n",
        "[nn.Dropout](https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout),\n",
        "[torch.arange](https://pytorch.org/docs/stable/torch.html#torch.arange),\n",
        "[torch.unsqueeze](https://pytorch.org/docs/stable/torch.html#torch.unsqueeze),\n",
        "[torch.Tensor.expand_as](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.expand_as),\n",
        "[torch.zeros_like](https://pytorch.org/docs/stable/torch.html#torch.zeros_like)"
      ]
    },
    {
      "metadata": {
        "id": "3H5EdR0JRljh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertEmbeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertEmbeddings, self).__init__()\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
        "        # any TensorFlow checkpoint file\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None):\n",
        "        seq_length = input_ids.size(1)\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device) ## [1, seq_length]\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids) ## [[1, seq_length]]\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        words_embeddings = self.word_embeddings(input_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids) ## token_type의 의미는???\n",
        "\n",
        "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qlrVyA6fiKxY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BertSelfAttention class 정의\n",
        "\n",
        "[torch.nn.Linear](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear)\n",
        "[torch.Tensor.view](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view),\n",
        "[torch.Tensor.permute](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute),\n",
        "[torch.transpose](https://pytorch.org/docs/stable/torch.html#torch.transpose),\n",
        "[torch.nn.Softmax](https://pytorch.org/docs/stable/nn.html#torch.nn.Softmax),\n",
        "[torch.matmul](https://pytorch.org/docs/stable/torch.html#torch.matmul),\n",
        "[torch.Tensor.contiguous](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.contiguous)"
      ]
    },
    {
      "metadata": {
        "id": "Qdu36p-piMBc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertSelfAttention, self).__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size ## config.hidden_size와 동일한 것 아닌가?\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size) ## shape 계산이 안됨\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3) ## shape 변경\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        mixed_query_layer = self.query(hidden_states) ## [-1, all_head_size]\n",
        "        mixed_key_layer = self.key(hidden_states) ## [-1, all_head_size]\n",
        "        mixed_value_layer = self.value(hidden_states) ## [-1, all_head_size]\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "        attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous() ## contiguous: make single block\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "        return context_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nUxXlQatzRi-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BertSelfOutput class 정의"
      ]
    },
    {
      "metadata": {
        "id": "MWaTt9XqoDI7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertSelfOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertSelfOutput, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "df6zbKrFz4px",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BertAttention class 정의\n",
        "### input_tensor, attention_mask는 무엇인가?"
      ]
    },
    {
      "metadata": {
        "id": "cDbYkI3loElq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertAttention, self).__init__()\n",
        "        self.self = BertSelfAttention(config)\n",
        "        self.output = BertSelfOutput(config)\n",
        "\n",
        "    def forward(self, input_tensor, attention_mask):\n",
        "        self_output = self.self(input_tensor, attention_mask)\n",
        "        attention_output = self.output(self_output, input_tensor)\n",
        "        return attention_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AsI_Z-up14JF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BertIntermediate class 정의"
      ]
    },
    {
      "metadata": {
        "id": "Pbs0ht29oKV3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertIntermediate, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.intermediate_act_fn = ACT2FN[config.hidden_act] \\\n",
        "            if isinstance(config.hidden_act, str) else config.hidden_act\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J_2DhDfMsmsP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BertOutput class 정의\n",
        "\n",
        "BertSelfOutput과 유사 input shape만 다름"
      ]
    },
    {
      "metadata": {
        "id": "LSGBWAkOoL1A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertOutput, self).__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TRsqBkJFs7AG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BertLayer class 정의"
      ]
    },
    {
      "metadata": {
        "id": "ndRQERmFoRCs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertLayer, self).__init__()\n",
        "        self.attention = BertAttention(config)\n",
        "        self.intermediate = BertIntermediate(config)\n",
        "        self.output = BertOutput(config)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        attention_output = self.attention(hidden_states, attention_mask)\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        return layer_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eTQkeg0w7KmB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BertEncoder class 정의\n",
        "\n",
        "[torch.nn.ModuleList](https://pytorch.org/docs/stable/nn.html#torch.nn.ModuleList)"
      ]
    },
    {
      "metadata": {
        "id": "5YF09zHEoUUW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertEncoder, self).__init__()\n",
        "        layer = BertLayer(config)\n",
        "        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True):\n",
        "        all_encoder_layers = []\n",
        "        for layer_module in self.layer:\n",
        "            hidden_states = layer_module(hidden_states, attention_mask)\n",
        "            if output_all_encoded_layers:\n",
        "                all_encoder_layers.append(hidden_states) ## 전체 Layer\n",
        "        if not output_all_encoded_layers:\n",
        "            all_encoder_layers.append(hidden_states) ## 마지막 Layer\n",
        "        return all_encoder_layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MmU1w3Yw_QxO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BertPooler class 정의"
      ]
    },
    {
      "metadata": {
        "id": "X_3RToy1oX_-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertPooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertPooler, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FjB7KGP4_fnX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BertPredictionHeadTransform class 정의"
      ]
    },
    {
      "metadata": {
        "id": "K2nxKn-nocpC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertPredictionHeadTransform(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertPredictionHeadTransform, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.transform_act_fn = ACT2FN[config.hidden_act] \\\n",
        "            if isinstance(config.hidden_act, str) else config.hidden_act\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.transform_act_fn(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states)\n",
        "        return hidden_states"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uLFKPUH6_ygW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BertLMPredictionHead class 정의\n",
        "\n",
        "[torch.nn.Parameter](https://pytorch.org/docs/stable/nn.html#torch.nn.Parameter)"
      ]
    },
    {
      "metadata": {
        "id": "seeGFZLkog0o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertLMPredictionHead(nn.Module):\n",
        "    def __init__(self, config, bert_model_embedding_weights):\n",
        "        super(BertLMPredictionHead, self).__init__()\n",
        "        self.transform = BertPredictionHeadTransform(config)\n",
        "\n",
        "        # The output weights are the same as the input embeddings, but there is\n",
        "        # an output-only bias for each token.\n",
        "        self.decoder = nn.Linear(bert_model_embedding_weights.size(1),\n",
        "                                 bert_model_embedding_weights.size(0),\n",
        "                                 bias=False)\n",
        "        self.decoder.weight = bert_model_embedding_weights\n",
        "        self.bias = nn.Parameter(torch.zeros(bert_model_embedding_weights.size(0)))\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.transform(hidden_states)\n",
        "        hidden_states = self.decoder(hidden_states) + self.bias\n",
        "        return hidden_states"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qsJZnc8gA8lU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BertOnlyMLMHead class 정의"
      ]
    },
    {
      "metadata": {
        "id": "_BOjMQKfokMY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertOnlyMLMHead(nn.Module):\n",
        "    def __init__(self, config, bert_model_embedding_weights):\n",
        "        super(BertOnlyMLMHead, self).__init__()\n",
        "        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\n",
        "\n",
        "    def forward(self, sequence_output):\n",
        "        prediction_scores = self.predictions(sequence_output)\n",
        "        return prediction_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PNlnQXQPBAWq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BertOnlyNSPHead class 정의"
      ]
    },
    {
      "metadata": {
        "id": "NaiYGd97onQx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertOnlyNSPHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertOnlyNSPHead, self).__init__()\n",
        "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, pooled_output):\n",
        "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
        "        return seq_relationship_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mWMZdB_yBhkd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BertPreTrainingHeads class 정의"
      ]
    },
    {
      "metadata": {
        "id": "V9SP8KQnoqUI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertPreTrainingHeads(nn.Module):\n",
        "    def __init__(self, config, bert_model_embedding_weights):\n",
        "        super(BertPreTrainingHeads, self).__init__()\n",
        "        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\n",
        "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, sequence_output, pooled_output):\n",
        "        prediction_scores = self.predictions(sequence_output)\n",
        "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
        "        return prediction_scores, seq_relationship_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WywMz0L_Eylk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## PreTrainedBertModel class 정의\n",
        "\n",
        "[torch.load](https://pytorch.org/docs/stable/torch.html#torch.load),\n",
        "[getattr](https://www.programiz.com/python-programming/methods/built-in/getattr)"
      ]
    },
    {
      "metadata": {
        "id": "0uRgg9oLov4A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class PreTrainedBertModel(nn.Module):\n",
        "    \"\"\" An abstract class to handle weights initialization and\n",
        "        a simple interface for dowloading and loading pretrained models.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, *inputs, **kwargs):\n",
        "        super(PreTrainedBertModel, self).__init__()\n",
        "        if not isinstance(config, BertConfig):\n",
        "            raise ValueError(\n",
        "                \"Parameter config in `{}(config)` should be an instance of class `BertConfig`. \"\n",
        "                \"To create a model from a Google pretrained model use \"\n",
        "                \"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(\n",
        "                    self.__class__.__name__, self.__class__.__name__\n",
        "                ))\n",
        "        self.config = config\n",
        "\n",
        "    def init_bert_weights(self, module):\n",
        "        \"\"\" Initialize the weights.\n",
        "        \"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "        elif isinstance(module, BertLayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name, state_dict=None, cache_dir=None, *inputs, **kwargs):\n",
        "        \"\"\"\n",
        "        Instantiate a PreTrainedBertModel from a pre-trained model file or a pytorch state dict.\n",
        "        Download and cache the pre-trained model file if needed.\n",
        "        Params:\n",
        "            pretrained_model_name: either:\n",
        "                - a str with the name of a pre-trained model to load selected in the list of:\n",
        "                    . `bert-base-uncased`\n",
        "                    . `bert-large-uncased`\n",
        "                    . `bert-base-cased`\n",
        "                    . `bert-large-cased`\n",
        "                    . `bert-base-multilingual-uncased`\n",
        "                    . `bert-base-multilingual-cased`\n",
        "                    . `bert-base-chinese`\n",
        "                - a path or url to a pretrained model archive containing:\n",
        "                    . `bert_config.json` a configuration file for the model\n",
        "                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\n",
        "            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n",
        "            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of Google pre-trained models\n",
        "            *inputs, **kwargs: additional input for the specific Bert class\n",
        "                (ex: num_labels for BertForSequenceClassification)\n",
        "        \"\"\"\n",
        "        ## 모델 선택\n",
        "        if pretrained_model_name in PRETRAINED_MODEL_ARCHIVE_MAP:\n",
        "            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name]\n",
        "        else:\n",
        "            archive_file = pretrained_model_name\n",
        "        # redirect to the cache, if necessary\n",
        "        ## 파일 조회\n",
        "        try:\n",
        "            resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)\n",
        "        except FileNotFoundError:\n",
        "            logger.error(\n",
        "                \"Model name '{}' was not found in model name list ({}). \"\n",
        "                \"We assumed '{}' was a path or url but couldn't find any file \"\n",
        "                \"associated to this path or url.\".format(\n",
        "                    pretrained_model_name,\n",
        "                    ', '.join(PRETRAINED_MODEL_ARCHIVE_MAP.keys()),\n",
        "                    archive_file))\n",
        "            return None\n",
        "        ## 로그\n",
        "        if resolved_archive_file == archive_file:\n",
        "            logger.info(\"loading archive file {}\".format(archive_file))\n",
        "        else:\n",
        "            logger.info(\"loading archive file {} from cache at {}\".format(\n",
        "                archive_file, resolved_archive_file))\n",
        "        tempdir = None\n",
        "        ## 파일 추출\n",
        "        if os.path.isdir(resolved_archive_file):\n",
        "            serialization_dir = resolved_archive_file\n",
        "        else:\n",
        "            # Extract archive to temp dir\n",
        "            tempdir = tempfile.mkdtemp()\n",
        "            logger.info(\"extracting archive file {} to temp dir {}\".format(\n",
        "                resolved_archive_file, tempdir))\n",
        "            with tarfile.open(resolved_archive_file, 'r:gz') as archive:\n",
        "                archive.extractall(tempdir)\n",
        "            serialization_dir = tempdir\n",
        "        # Load config\n",
        "        ## config 로딩\n",
        "        config_file = os.path.join(serialization_dir, CONFIG_NAME)\n",
        "        config = BertConfig.from_json_file(config_file)\n",
        "        logger.info(\"Model config {}\".format(config))\n",
        "        # Instantiate model.\n",
        "        ## 자신(PreTrainedBertModel) 생성\n",
        "        model = cls(config, *inputs, **kwargs)\n",
        "        if state_dict is None:\n",
        "            weights_path = os.path.join(serialization_dir, WEIGHTS_NAME)\n",
        "            ## pytorch_model.bin 로딩\n",
        "            state_dict = torch.load(weights_path)\n",
        "\n",
        "        ## key 값을 gamma -> weight, beta -> bias로 변경\n",
        "        old_keys = []\n",
        "        new_keys = []\n",
        "        for key in state_dict.keys():\n",
        "            new_key = None\n",
        "            if 'gamma' in key:\n",
        "                new_key = key.replace('gamma', 'weight')\n",
        "            if 'beta' in key:\n",
        "                new_key = key.replace('beta', 'bias')\n",
        "            if new_key:\n",
        "                old_keys.append(key)\n",
        "                new_keys.append(new_key)\n",
        "        for old_key, new_key in zip(old_keys, new_keys):\n",
        "            state_dict[new_key] = state_dict.pop(old_key)\n",
        "\n",
        "        missing_keys = []\n",
        "        unexpected_keys = []\n",
        "        error_msgs = []\n",
        "        # copy state_dict so _load_from_state_dict can modify it\n",
        "        ## metadata = state_dict._metadata와 동일함\n",
        "        metadata = getattr(state_dict, '_metadata', None)\n",
        "        state_dict = state_dict.copy()\n",
        "        if metadata is not None:\n",
        "            state_dict._metadata = metadata\n",
        "\n",
        "        ### Abstract 단에서는 아직 해석 안됨\n",
        "        def load(module, prefix=''):\n",
        "            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n",
        "            module._load_from_state_dict(\n",
        "                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n",
        "            for name, child in module._modules.items():\n",
        "                if child is not None:\n",
        "                    load(child, prefix + name + '.')\n",
        "        load(model, prefix='' if hasattr(model, 'bert') else 'bert.')\n",
        "        if len(missing_keys) > 0:\n",
        "            logger.info(\"Weights of {} not initialized from pretrained model: {}\".format(\n",
        "                model.__class__.__name__, missing_keys))\n",
        "        if len(unexpected_keys) > 0:\n",
        "            logger.info(\"Weights from pretrained model not used in {}: {}\".format(\n",
        "                model.__class__.__name__, unexpected_keys))\n",
        "        if tempdir:\n",
        "            # Clean up temp dir\n",
        "            shutil.rmtree(tempdir)\n",
        "        return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H1dpY-MaEcsJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BertModel clss 정의\n",
        "\n",
        "[torch.unsqueeze](https://pytorch.org/docs/stable/torch.html#torch.unsqueeze),\n",
        "[torch.Tensor.to](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.to),\n",
        "[torch.nn.Module.parameters](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.parameters)"
      ]
    },
    {
      "metadata": {
        "id": "EHGoyN0Uo1Nk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertModel(PreTrainedBertModel):\n",
        "    \"\"\"BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n",
        "    Params:\n",
        "        config: a BertConfig class instance with the configuration to build a new model\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "            a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n",
        "    Outputs: Tuple of (encoded_layers, pooled_output)\n",
        "        `encoded_layers`: controled by `output_all_encoded_layers` argument:\n",
        "            - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n",
        "                of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n",
        "                encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n",
        "            - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n",
        "                to the last attention block of shape [batch_size, sequence_length, hidden_size],\n",
        "        `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n",
        "            classifier pretrained on top of the hidden state associated to the first character of the\n",
        "            input (`CLF`) to train on the Next-Sentence task (see BERT's paper).\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
        "    config = modeling.BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "    model = modeling.BertModel(config=config)\n",
        "    all_encoder_layers, pooled_output = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertModel, self).__init__(config)\n",
        "        self.embeddings = BertEmbeddings(config)\n",
        "        self.encoder = BertEncoder(config)\n",
        "        self.pooler = BertPooler(config)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=True):\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_ids)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        # We create a 3D attention mask from a 2D tensor mask.\n",
        "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
        "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
        "        # this attention mask is more simple than the triangular masking of causal attention\n",
        "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "        # masked positions, this operation will create a tensor which is 0.0 for\n",
        "        # positions we want to attend and -10000.0 for masked positions.\n",
        "        # Since we are adding it to the raw scores before the softmax, this is\n",
        "        # effectively the same as removing these entirely.\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility ## long -> float로 변환\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0 ## [-10000.0 .. 0]값 -10000.0의 경우 softmax를 취할 경우 0으로 수렴 함 (mask 됨)\n",
        "\n",
        "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
        "        encoded_layers = self.encoder(embedding_output,\n",
        "                                      extended_attention_mask,\n",
        "                                      output_all_encoded_layers=output_all_encoded_layers)\n",
        "        sequence_output = encoded_layers[-1]\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "        if not output_all_encoded_layers:\n",
        "            encoded_layers = encoded_layers[-1]\n",
        "        return encoded_layers, pooled_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vxaxt7hCcJ-h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BertForPreTraining class 정의\n",
        "\n",
        "[torch.nn.CrossEntropyLoss](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss),\n",
        "[torch.Tensor.view](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view),\n"
      ]
    },
    {
      "metadata": {
        "id": "LffYBv52o5ki",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertForPreTraining(PreTrainedBertModel):\n",
        "    \"\"\"BERT model with pre-training heads.\n",
        "    This module comprises the BERT model followed by the two pre-training heads:\n",
        "        - the masked language modeling head, and\n",
        "        - the next sentence classification head.\n",
        "    Params:\n",
        "        config: a BertConfig class instance with the configuration to build a new model.\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "            a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `masked_lm_labels`: masked language modeling labels: torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with indices selected in [-1, 0, ..., vocab_size]. All labels set to -1 are ignored (masked), the loss\n",
        "            is only computed for the labels set in [0, ..., vocab_size]\n",
        "        `next_sentence_label`: next sentence classification loss: torch.LongTensor of shape [batch_size]\n",
        "            with indices selected in [0, 1].\n",
        "            0 => next sentence is the continuation, 1 => next sentence is a random sentence.\n",
        "    Outputs:\n",
        "        if `masked_lm_labels` and `next_sentence_label` are not `None`:\n",
        "            Outputs the total_loss which is the sum of the masked language modeling loss and the next\n",
        "            sentence classification loss.\n",
        "        if `masked_lm_labels` or `next_sentence_label` is `None`:\n",
        "            Outputs a tuple comprising\n",
        "            - the masked language modeling logits of shape [batch_size, sequence_length, vocab_size], and\n",
        "            - the next sentence classification logits of shape [batch_size, 2].\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
        "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "    model = BertForPreTraining(config)\n",
        "    masked_lm_logits_scores, seq_relationship_logits = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertForPreTraining, self).__init__(config)\n",
        "        self.bert = BertModel(config)\n",
        "        self.cls = BertPreTrainingHeads(config, self.bert.embeddings.word_embeddings.weight)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None, next_sentence_label=None):\n",
        "        sequence_output, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n",
        "                                                   output_all_encoded_layers=False)\n",
        "        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
        "\n",
        "        if masked_lm_labels is not None and next_sentence_label is not None:\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=-1)\n",
        "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1)) ## 각각 로스 계산\n",
        "            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1)) ## 각각 로스 계산\n",
        "            total_loss = masked_lm_loss + next_sentence_loss\n",
        "            return total_loss\n",
        "        else:\n",
        "            return prediction_scores, seq_relationship_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UHtU2MJBjSxg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BertForMaskedLM class 정의"
      ]
    },
    {
      "metadata": {
        "id": "QqJw0Bqbo9a_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertForMaskedLM(PreTrainedBertModel):\n",
        "    \"\"\"BERT model with the masked language modeling head.\n",
        "    This module comprises the BERT model followed by the masked language modeling head.\n",
        "    Params:\n",
        "        config: a BertConfig class instance with the configuration to build a new model.\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "            a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `masked_lm_labels`: masked language modeling labels: torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with indices selected in [-1, 0, ..., vocab_size]. All labels set to -1 are ignored (masked), the loss\n",
        "            is only computed for the labels set in [0, ..., vocab_size]\n",
        "    Outputs:\n",
        "        if `masked_lm_labels` is  not `None`:\n",
        "            Outputs the masked language modeling loss.\n",
        "        if `masked_lm_labels` is `None`:\n",
        "            Outputs the masked language modeling logits of shape [batch_size, sequence_length, vocab_size].\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
        "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "    model = BertForMaskedLM(config)\n",
        "    masked_lm_logits_scores = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertForMaskedLM, self).__init__(config)\n",
        "        self.bert = BertModel(config)\n",
        "        self.cls = BertOnlyMLMHead(config, self.bert.embeddings.word_embeddings.weight)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None):\n",
        "        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask,\n",
        "                                       output_all_encoded_layers=False)\n",
        "        prediction_scores = self.cls(sequence_output)\n",
        "\n",
        "        if masked_lm_labels is not None:\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=-1)\n",
        "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n",
        "            return masked_lm_loss\n",
        "        else:\n",
        "            return prediction_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F3auRaHH4-E3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BertForNextSentencePrediction class 정의"
      ]
    },
    {
      "metadata": {
        "id": "WaiMRXglpBl_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertForNextSentencePrediction(PreTrainedBertModel):\n",
        "    \"\"\"BERT model with next sentence prediction head.\n",
        "    This module comprises the BERT model followed by the next sentence classification head.\n",
        "    Params:\n",
        "        config: a BertConfig class instance with the configuration to build a new model.\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "            a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `next_sentence_label`: next sentence classification loss: torch.LongTensor of shape [batch_size]\n",
        "            with indices selected in [0, 1].\n",
        "            0 => next sentence is the continuation, 1 => next sentence is a random sentence.\n",
        "    Outputs:\n",
        "        if `next_sentence_label` is not `None`:\n",
        "            Outputs the total_loss which is the sum of the masked language modeling loss and the next\n",
        "            sentence classification loss.\n",
        "        if `next_sentence_label` is `None`:\n",
        "            Outputs the next sentence classification logits of shape [batch_size, 2].\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
        "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "    model = BertForNextSentencePrediction(config)\n",
        "    seq_relationship_logits = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertForNextSentencePrediction, self).__init__(config)\n",
        "        self.bert = BertModel(config)\n",
        "        self.cls = BertOnlyNSPHead(config)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, next_sentence_label=None):\n",
        "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n",
        "                                     output_all_encoded_layers=False)\n",
        "        seq_relationship_score = self.cls( pooled_output)\n",
        "\n",
        "        if next_sentence_label is not None:\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=-1)\n",
        "            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n",
        "            return next_sentence_loss\n",
        "        else:\n",
        "            return seq_relationship_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wLvlJ9Sh-z9C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BertForSequenceClassification class 정의"
      ]
    },
    {
      "metadata": {
        "id": "F7A9keqkpFhn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertForSequenceClassification(PreTrainedBertModel):\n",
        "    \"\"\"BERT model for classification.\n",
        "    This module is composed of the BERT model with a linear layer on top of\n",
        "    the pooled output.\n",
        "    Params:\n",
        "        `config`: a BertConfig class instance with the configuration to build a new model.\n",
        "        `num_labels`: the number of classes for the classifier. Default = 2.\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "            a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n",
        "            with indices selected in [0, ..., num_labels].\n",
        "    Outputs:\n",
        "        if `labels` is not `None`:\n",
        "            Outputs the CrossEntropy classification loss of the output with the labels.\n",
        "        if `labels` is `None`:\n",
        "            Outputs the classification logits of shape [batch_size, num_labels].\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
        "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "    num_labels = 2\n",
        "    model = BertForSequenceClassification(config, num_labels)\n",
        "    logits = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config, num_labels=2):\n",
        "        super(BertForSequenceClassification, self).__init__(config)\n",
        "        self.num_labels = num_labels\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
        "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            return loss\n",
        "        else:\n",
        "            return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ryaP0OAwAAHE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BertForMultipleChoice class 정의"
      ]
    },
    {
      "metadata": {
        "id": "e06GJOVIpKEo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertForMultipleChoice(PreTrainedBertModel):\n",
        "    \"\"\"BERT model for multiple choice tasks.\n",
        "    This module is composed of the BERT model with a linear layer on top of\n",
        "    the pooled output.\n",
        "    Params:\n",
        "        `config`: a BertConfig class instance with the configuration to build a new model.\n",
        "        `num_choices`: the number of classes for the classifier. Default = 2.\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, num_choices, sequence_length]\n",
        "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, num_choices, sequence_length]\n",
        "            with the token types indices selected in [0, 1]. Type 0 corresponds to a `sentence A`\n",
        "            and type 1 corresponds to a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, num_choices, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n",
        "            with indices selected in [0, ..., num_choices].\n",
        "    Outputs:\n",
        "        if `labels` is not `None`:\n",
        "            Outputs the CrossEntropy classification loss of the output with the labels.\n",
        "        if `labels` is `None`:\n",
        "            Outputs the classification logits of shape [batch_size, num_labels].\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[[31, 51, 99], [15, 5, 0]], [[12, 16, 42], [14, 28, 57]]])\n",
        "    input_mask = torch.LongTensor([[[1, 1, 1], [1, 1, 0]],[[1,1,0], [1, 0, 0]]])\n",
        "    token_type_ids = torch.LongTensor([[[0, 0, 1], [0, 1, 0]],[[0, 1, 1], [0, 0, 1]]])\n",
        "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "    num_choices = 2\n",
        "    model = BertForMultipleChoice(config, num_choices)\n",
        "    logits = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config, num_choices=2):\n",
        "        super(BertForMultipleChoice, self).__init__(config)\n",
        "        self.num_choices = num_choices\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
        "        flat_input_ids = input_ids.view(-1, input_ids.size(-1))\n",
        "        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n",
        "        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1))\n",
        "        _, pooled_output = self.bert(flat_input_ids, flat_token_type_ids, flat_attention_mask, output_all_encoded_layers=False)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        reshaped_logits = logits.view(-1, self.num_choices)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(reshaped_logits, labels)\n",
        "            return loss\n",
        "        else:\n",
        "            return reshaped_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O4qbB_AMA_K9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BertForTokenClassification class 정의"
      ]
    },
    {
      "metadata": {
        "id": "4okDqg4_pPl5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertForTokenClassification(PreTrainedBertModel):\n",
        "    \"\"\"BERT model for token-level classification.\n",
        "    This module is composed of the BERT model with a linear layer on top of\n",
        "    the full hidden state of the last layer.\n",
        "    Params:\n",
        "        `config`: a BertConfig class instance with the configuration to build a new model.\n",
        "        `num_labels`: the number of classes for the classifier. Default = 2.\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "            a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with indices selected in [0, ..., num_labels].\n",
        "    Outputs:\n",
        "        if `labels` is not `None`:\n",
        "            Outputs the CrossEntropy classification loss of the output with the labels.\n",
        "        if `labels` is `None`:\n",
        "            Outputs the classification logits of shape [batch_size, sequence_length, num_labels].\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
        "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "    num_labels = 2\n",
        "    model = BertForTokenClassification(config, num_labels)\n",
        "    logits = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config, num_labels=2):\n",
        "        super(BertForTokenClassification, self).__init__(config)\n",
        "        self.num_labels = num_labels\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
        "        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            # Only keep active parts of the loss\n",
        "            if attention_mask is not None:\n",
        "                active_loss = attention_mask.view(-1) == 1\n",
        "                active_logits = logits.view(-1, self.num_labels)[active_loss]\n",
        "                active_labels = labels.view(-1)[active_loss]\n",
        "                loss = loss_fct(active_logits, active_labels)\n",
        "            else:\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            return loss\n",
        "        else:\n",
        "            return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lxBwbbPUpWgo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertForQuestionAnswering(PreTrainedBertModel):\n",
        "    \"\"\"BERT model for Question Answering (span extraction).\n",
        "    This module is composed of the BERT model with a linear layer on top of\n",
        "    the sequence output that computes start_logits and end_logits\n",
        "    Params:\n",
        "        `config`: a BertConfig class instance with the configuration to build a new model.\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "            a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `start_positions`: position of the first token for the labeled span: torch.LongTensor of shape [batch_size].\n",
        "            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\n",
        "            into account for computing the loss.\n",
        "        `end_positions`: position of the last token for the labeled span: torch.LongTensor of shape [batch_size].\n",
        "            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\n",
        "            into account for computing the loss.\n",
        "    Outputs:\n",
        "        if `start_positions` and `end_positions` are not `None`:\n",
        "            Outputs the total_loss which is the sum of the CrossEntropy loss for the start and end token positions.\n",
        "        if `start_positions` or `end_positions` is `None`:\n",
        "            Outputs a tuple of start_logits, end_logits which are the logits respectively for the start and end\n",
        "            position tokens of shape [batch_size, sequence_length].\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
        "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "    model = BertForQuestionAnswering(config)\n",
        "    start_logits, end_logits = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertForQuestionAnswering, self).__init__(config)\n",
        "        self.bert = BertModel(config)\n",
        "        # TODO check with Google if it's normal there is no dropout on the token classifier of SQuAD in the TF version\n",
        "        # self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, start_positions=None, end_positions=None):\n",
        "        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
        "        logits = self.qa_outputs(sequence_output)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            # If we are on multi-GPU, split add a dimension\n",
        "            if len(start_positions.size()) > 1:\n",
        "                start_positions = start_positions.squeeze(-1)\n",
        "            if len(end_positions.size()) > 1:\n",
        "                end_positions = end_positions.squeeze(-1)\n",
        "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
        "            ignored_index = start_logits.size(1)\n",
        "            start_positions.clamp_(0, ignored_index)\n",
        "            end_positions.clamp_(0, ignored_index)\n",
        "\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
        "            start_loss = loss_fct(start_logits, start_positions)\n",
        "            end_loss = loss_fct(end_logits, end_positions)\n",
        "            total_loss = (start_loss + end_loss) / 2\n",
        "            return total_loss\n",
        "        else:\n",
        "            return start_logits, end_logits"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}