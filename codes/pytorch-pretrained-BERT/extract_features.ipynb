{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "extract_features.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "90kTb1anDnAQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "python -m examples.extract_features \\\n",
        "  --bert_model bert-base-uncased \\\n",
        "  --do_lower_case \\\n",
        "  --input_file ./samples/input.txt \\\n",
        "  --output_file ./samples/input_out.txt \\\n",
        "  --max_seq_length 128"
      ]
    },
    {
      "metadata": {
        "id": "aT3twNTFBfkN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import collections\n",
        "import logging\n",
        "import json\n",
        "import re\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
        "from pytorch_pretrained_bert.modeling import BertModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VLTB28FTBmM2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s', \n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gaSRidkcBofL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class InputExample(object):\n",
        "\n",
        "    def __init__(self, unique_id, text_a, text_b):\n",
        "        self.unique_id = unique_id\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yH8COHf5BrdT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, unique_id, tokens, input_ids, input_mask, input_type_ids):\n",
        "        self.unique_id = unique_id\n",
        "        self.tokens = tokens\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.input_type_ids = input_type_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iBWqtbcSBuKs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert_examples_to_features(examples, seq_length, tokenizer):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        tokens_a = tokenizer.tokenize(example.text_a)\n",
        "\n",
        "        tokens_b = None\n",
        "        if example.text_b:\n",
        "            tokens_b = tokenizer.tokenize(example.text_b)\n",
        "\n",
        "        if tokens_b:\n",
        "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "            # length is less than the specified length.\n",
        "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "            _truncate_seq_pair(tokens_a, tokens_b, seq_length - 3)\n",
        "        else:\n",
        "            # Account for [CLS] and [SEP] with \"- 2\"\n",
        "            if len(tokens_a) > seq_length - 2:\n",
        "                tokens_a = tokens_a[0:(seq_length - 2)]\n",
        "\n",
        "        # The convention in BERT is:\n",
        "        # (a) For sequence pairs:\n",
        "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "        #  type_ids:   0   0  0    0    0     0      0   0    1  1  1   1  1   1\n",
        "        # (b) For single sequences:\n",
        "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "        #  type_ids:   0   0   0   0  0     0   0\n",
        "        #\n",
        "        # Where \"type_ids\" are used to indicate whether this is the first\n",
        "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "        # embedding vector (and position vector). This is not *strictly* necessary\n",
        "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
        "        # it easier for the model to learn the concept of sequences.\n",
        "        #\n",
        "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "        # the entire model is fine-tuned.\n",
        "        tokens = []\n",
        "        input_type_ids = []\n",
        "        tokens.append(\"[CLS]\")\n",
        "        input_type_ids.append(0)\n",
        "        for token in tokens_a:\n",
        "            tokens.append(token)\n",
        "            input_type_ids.append(0)\n",
        "        tokens.append(\"[SEP]\")\n",
        "        input_type_ids.append(0)\n",
        "\n",
        "        if tokens_b:\n",
        "            for token in tokens_b:\n",
        "                tokens.append(token)\n",
        "                input_type_ids.append(1)\n",
        "            tokens.append(\"[SEP]\")\n",
        "            input_type_ids.append(1)\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        while len(input_ids) < seq_length:\n",
        "            input_ids.append(0)\n",
        "            input_mask.append(0)\n",
        "            input_type_ids.append(0)\n",
        "\n",
        "        assert len(input_ids) == seq_length\n",
        "        assert len(input_mask) == seq_length\n",
        "        assert len(input_type_ids) == seq_length\n",
        "\n",
        "        if ex_index < 5:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"unique_id: %s\" % (example.unique_id))\n",
        "            logger.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
        "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "            logger.info(\n",
        "                \"input_type_ids: %s\" % \" \".join([str(x) for x in input_type_ids]))\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                unique_id=example.unique_id,\n",
        "                tokens=tokens,\n",
        "                input_ids=input_ids,\n",
        "                input_mask=input_mask,\n",
        "                input_type_ids=input_type_ids))\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9lNEDJn3B6jd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "    # This is a simple heuristic which will always truncate the longer sequence\n",
        "    # one token at a time. This makes more sense than truncating an equal percent\n",
        "    # of tokens from each, since if one sequence is very short then each token\n",
        "    # that's truncated likely contains more information than a longer sequence.\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_g7eCJ9dB-I0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_examples(input_file):\n",
        "    \"\"\"Read a list of `InputExample`s from an input file.\"\"\"\n",
        "    examples = []\n",
        "    unique_id = 0\n",
        "    with open(input_file, \"r\", encoding='utf-8') as reader:\n",
        "        while True:\n",
        "            line = reader.readline()\n",
        "            if not line:\n",
        "                break\n",
        "            line = line.strip()\n",
        "            text_a = None\n",
        "            text_b = None\n",
        "            m = re.match(r\"^(.*) \\|\\|\\| (.*)$\", line)\n",
        "            if m is None:\n",
        "                text_a = line\n",
        "            else:\n",
        "                text_a = m.group(1)\n",
        "                text_b = m.group(2)\n",
        "            examples.append(\n",
        "                InputExample(unique_id=unique_id, text_a=text_a, text_b=text_b))\n",
        "            unique_id += 1\n",
        "    return examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4Qc8n--THgG0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Main 함수\n",
        "\n",
        "[torch.utils.data.TensorDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset),\n",
        "[torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader),\n",
        "[torch.utils.data.SequentialSampler](https://pytorch.org/docs/stable/data.html#torch.utils.data.SequentialSampler),\n",
        "[torch.utils.data.distributed.DistributedSampler](https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler),\n",
        "[torch.arange](https://pytorch.org/docs/stable/torch.html#torch.arange),\n",
        "[torch.Tensor.detach](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.detach),\n",
        "[torch.Tensor.cpu](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.cpu),\n",
        "[torch.Tensor.numpy](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.numpy)"
      ]
    },
    {
      "metadata": {
        "id": "qiO8mjiYCXqL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    ## Required parameters\n",
        "    parser.add_argument(\"--input_file\", default=None, type=str, required=True)\n",
        "    parser.add_argument(\"--output_file\", default=None, type=str, required=True)\n",
        "    parser.add_argument(\"--bert_model\", default=None, type=str, required=True,\n",
        "                        help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n",
        "                             \"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\")\n",
        "\n",
        "    ## Other parameters\n",
        "    parser.add_argument(\"--do_lower_case\", action='store_true', help=\"Set this flag if you are using an uncased model.\")\n",
        "    parser.add_argument(\"--layers\", default=\"-1,-2,-3,-4\", type=str)\n",
        "    parser.add_argument(\"--max_seq_length\", default=128, type=int,\n",
        "                        help=\"The maximum total input sequence length after WordPiece tokenization. Sequences longer \"\n",
        "                            \"than this will be truncated, and sequences shorter than this will be padded.\")\n",
        "    parser.add_argument(\"--batch_size\", default=32, type=int, help=\"Batch size for predictions.\")\n",
        "    parser.add_argument(\"--local_rank\",\n",
        "                        type=int,\n",
        "                        default=-1,\n",
        "                        help = \"local_rank for distributed training on gpus\")\n",
        "    parser.add_argument(\"--no_cuda\",\n",
        "                        action='store_true',\n",
        "                        help=\"Whether not to use CUDA when available\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.local_rank == -1 or args.no_cuda:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "        n_gpu = torch.cuda.device_count()\n",
        "    else:\n",
        "        device = torch.device(\"cuda\", args.local_rank)\n",
        "        n_gpu = 1\n",
        "        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "        torch.distributed.init_process_group(backend='nccl')\n",
        "    logger.info(\"device: {} n_gpu: {} distributed training: {}\".format(device, n_gpu, bool(args.local_rank != -1)))\n",
        "\n",
        "    layer_indexes = [int(x) for x in args.layers.split(\",\")]\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n",
        "\n",
        "    examples = read_examples(args.input_file)\n",
        "\n",
        "    features = convert_examples_to_features(\n",
        "        examples=examples, seq_length=args.max_seq_length, tokenizer=tokenizer)\n",
        "\n",
        "    unique_id_to_feature = {}\n",
        "    for feature in features:\n",
        "        unique_id_to_feature[feature.unique_id] = feature\n",
        "\n",
        "    model = BertModel.from_pretrained(args.bert_model)\n",
        "    model.to(device)\n",
        "\n",
        "    if args.local_rank != -1:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n",
        "                                                          output_device=args.local_rank)\n",
        "    elif n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
        "    all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
        "\n",
        "    eval_data = TensorDataset(all_input_ids, all_input_mask, all_example_index)\n",
        "    if args.local_rank == -1:\n",
        "        eval_sampler = SequentialSampler(eval_data)\n",
        "    else:\n",
        "        eval_sampler = DistributedSampler(eval_data)\n",
        "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.batch_size)\n",
        "\n",
        "    model.eval()\n",
        "    with open(args.output_file, \"w\", encoding='utf-8') as writer:\n",
        "        for input_ids, input_mask, example_indices in eval_dataloader:\n",
        "            input_ids = input_ids.to(device)\n",
        "            input_mask = input_mask.to(device)\n",
        "\n",
        "            all_encoder_layers, _ = model(input_ids, token_type_ids=None, attention_mask=input_mask)\n",
        "            all_encoder_layers = all_encoder_layers\n",
        "\n",
        "            for b, example_index in enumerate(example_indices):\n",
        "                feature = features[example_index.item()]\n",
        "                unique_id = int(feature.unique_id)\n",
        "                # feature = unique_id_to_feature[unique_id]\n",
        "                output_json = collections.OrderedDict()\n",
        "                output_json[\"linex_index\"] = unique_id\n",
        "                all_out_features = []\n",
        "                for (i, token) in enumerate(feature.tokens):\n",
        "                    all_layers = []\n",
        "                    for (j, layer_index) in enumerate(layer_indexes):\n",
        "                        layer_output = all_encoder_layers[int(layer_index)].detach().cpu().numpy()\n",
        "                        layer_output = layer_output[b]\n",
        "                        layers = collections.OrderedDict()\n",
        "                        layers[\"index\"] = layer_index\n",
        "                        layers[\"values\"] = [\n",
        "                            round(x.item(), 6) for x in layer_output[i]\n",
        "                        ]\n",
        "                        all_layers.append(layers)\n",
        "                    out_features = collections.OrderedDict()\n",
        "                    out_features[\"token\"] = token\n",
        "                    out_features[\"layers\"] = all_layers\n",
        "                    all_out_features.append(out_features)\n",
        "                output_json[\"features\"] = all_out_features\n",
        "                writer.write(json.dumps(output_json) + \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}